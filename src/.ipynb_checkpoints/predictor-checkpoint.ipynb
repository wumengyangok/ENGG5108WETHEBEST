{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "# tqdm\n",
    "import tqdm\n",
    "from tqdm import tqdm_notebook, trange\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, TensorDataset)\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM, BertForSequenceClassification, AdamW\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# locals\n",
    "from config import Config\n",
    "from input_example import InputExample\n",
    "from input_features import InputFeatures, convert_example_to_feature\n",
    "from twitter import Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# The maximum total input sequence length after WordPiece tokenization.\n",
    "# Sequences longer than this will be truncated, and sequences shorter than this will be padded.\n",
    "MAX_SEQ_LENGTH = 128\n",
    "\n",
    "TRAIN_BATCH_SIZE = 24\n",
    "EVAL_BATCH_SIZE = 8\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_TRAIN_EPOCHS = 1\n",
    "RANDOM_SEED = 42\n",
    "GRADIENT_ACCUMULATION_STEPS = 1\n",
    "WARMUP_PROPORTION = 0.1\n",
    "OUTPUT_MODE = 'classification'\n",
    "\n",
    "CONFIG_NAME = \"config.json\"\n",
    "WEIGHTS_NAME = \"pytorch_model.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.twitter = Twitter()\n",
    "        self.config = Config()\n",
    "    \n",
    "    def set_stock(self, stock):\n",
    "        self.stock = stock\n",
    "    \n",
    "    def set_date_range(self, from_date, to_date):\n",
    "        self.to_date = datetime.strptime(to_date, \"%Y-%m-%d\")\n",
    "        self.from_date = datetime.strptime(from_date, \"%Y-%m-%d\")\n",
    "        \n",
    "    def set_model(self, filename = None):\n",
    "        if filename == None:\n",
    "            self.model = BertForSequenceClassification.from_pretrained(\n",
    "                'bert-base-cased', cache_dir=self.config.get_cache_dir(), num_labels=2\n",
    "            )\n",
    "        else:\n",
    "            self.model = BertForSequenceClassification.from_pretrained(\n",
    "                os.path.join(self.config.get_cache_dir(), filename), cache_dir=self.config.get_cache_dir(), num_labels=2\n",
    "            )\n",
    "            \n",
    "        self.model.to(device)\n",
    "        \n",
    "    def load_data(self, query = None, count = 10):\n",
    "        if query == None:\n",
    "            query = f\"${self.stock}\"\n",
    "            \n",
    "        tweets = self.twitter.get_online_tweets(query, self.from_date.strftime(\"%Y-%m-%d\"), self.to_date.strftime(\"%Y-%m-%d\"), count)\n",
    "        features = self.twitter.conv2features(tweets)\n",
    "        \n",
    "        all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "        all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "        all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "        \n",
    "        if OUTPUT_MODE == \"classification\":\n",
    "            all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n",
    "        elif OUTPUT_MODE == \"regression\":\n",
    "            all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.float)\n",
    "            \n",
    "        self.eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "        \n",
    "    def classify_tweets(self):\n",
    "        # Run prediction for full data\n",
    "        eval_sampler = SequentialSampler(self.eval_data)\n",
    "        eval_dataloader = DataLoader(self.eval_data, sampler=eval_sampler, batch_size=EVAL_BATCH_SIZE)\n",
    "        \n",
    "        # Start predicts stage\n",
    "        self.model.eval()\n",
    "        eval_loss = 0\n",
    "        nb_eval_steps = 0\n",
    "        preds = []\n",
    "        \n",
    "        for input_ids, input_mask, segment_ids, label_ids in tqdm_notebook(eval_dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = input_ids.to(device)\n",
    "            input_mask = input_mask.to(device)\n",
    "            segment_ids = segment_ids.to(device)\n",
    "            label_ids = label_ids.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = self.model(input_ids, segment_ids, input_mask, labels=None)\n",
    "                \n",
    "            logits = output[0].detach().cpu().numpy()\n",
    "            \n",
    "            if len(preds) == 0:\n",
    "                preds.append(logits)\n",
    "            else:\n",
    "                preds[0] = np.append(preds[0], logits, axis=0)\n",
    "                \n",
    "        preds = preds[0]\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieve twitter credential: ../twitter-cred\\credentials.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\ivangundampc/.cache\\torch\\hub\\huggingface_pytorch-transformers_master\n",
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at C:\\Users\\ivangundampc\\.cache\\torch\\transformers\\5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at ../cache/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.d7a3af18ce3a2ab7c0f48f04dc8daff45ed9a3ed333b9e9a79d012a0dedf87a6\n",
      "INFO:transformers.configuration_utils:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at ../cache/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2\n",
      "INFO:transformers.modeling_utils:Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "INFO:transformers.modeling_utils:Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n"
     ]
    }
   ],
   "source": [
    "predictor = Predictor()\n",
    "predictor.set_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af282be6f99d4c99a9b27c846ec7a144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=7, style=ProgressStyle(description_width='inâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[-0.12702045  0.2871619 ]\n",
      " [ 0.12181091  0.34446043]\n",
      " [-0.04136693  0.21541844]\n",
      " [ 0.06951762  0.31560788]\n",
      " [ 0.06356005  0.32711464]\n",
      " [-0.04136693  0.21541844]\n",
      " [-0.04136693  0.21541844]\n",
      " [-0.04136693  0.21541844]\n",
      " [-0.04136693  0.21541844]\n",
      " [-0.00856753  0.21816802]\n",
      " [ 0.01549479  0.26508442]\n",
      " [-0.01316893  0.2843372 ]\n",
      " [-0.00220382  0.16379139]\n",
      " [ 0.07010119  0.2905017 ]\n",
      " [-0.14275637  0.27899   ]\n",
      " [ 0.03465731  0.27055955]\n",
      " [ 0.07325664  0.3143697 ]\n",
      " [-0.06785884  0.25928167]\n",
      " [ 0.01559635  0.30415982]\n",
      " [ 0.09444959  0.3143896 ]\n",
      " [-0.01773433  0.24542034]\n",
      " [ 0.18109378  0.2981025 ]\n",
      " [-0.00061217  0.29476646]\n",
      " [-0.0136187   0.29402113]\n",
      " [-0.03233413  0.25972164]\n",
      " [ 0.08239074  0.1640906 ]\n",
      " [ 0.13346691  0.20968746]\n",
      " [-0.04789746  0.3101174 ]\n",
      " [-0.061235    0.18748933]\n",
      " [ 0.21715312  0.3103689 ]\n",
      " [ 0.08565958  0.30528954]\n",
      " [-0.05189344  0.2944364 ]\n",
      " [ 0.30211806  0.26764444]\n",
      " [ 0.09242627  0.3356067 ]\n",
      " [-0.00258614  0.1888335 ]\n",
      " [ 0.02731377  0.24190482]\n",
      " [-0.0336304   0.1195385 ]\n",
      " [ 0.00272653  0.3554326 ]\n",
      " [ 0.06638867  0.27973217]\n",
      " [ 0.22986111  0.20474751]\n",
      " [ 0.22986111  0.20474751]\n",
      " [-0.02715671  0.26114008]\n",
      " [ 0.161625    0.3026379 ]\n",
      " [-0.07690072  0.31333786]\n",
      " [ 0.08447032  0.2532533 ]\n",
      " [ 0.18196347  0.3824399 ]\n",
      " [ 0.28904825  0.29532674]\n",
      " [ 0.03910465  0.31824014]\n",
      " [-0.05076717  0.18719205]\n",
      " [ 0.04328919  0.27468956]]\n"
     ]
    }
   ],
   "source": [
    "predictor.set_stock(\"AAPL\")\n",
    "predictor.set_date_range(\"2019-12-06\", \"2019-12-07\")\n",
    "predictor.load_data(count = 50)\n",
    "print(predictor.classify_tweets())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
